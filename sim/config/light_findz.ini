[DEFAULT]

# ====== Must-Have ======
# These parameters are required by the pipeline, regardless of your custom code

# ------ Data ------
dataset = light_latlng
no_batch = True
# bs = 4
cache = True

# ------ Model ------
model = light_findz

# ------ Optimization ------
loss = l2
lr = 1e-4
lr_decay_steps = 500_000
lr_decay_rate = 0.1
clipnorm = -1
clipvalue = -1
epochs = 100

# ------ Logging and Checkpointing ------
ckpt_period = 1
vali_period = 1
vali_batches = 1
vis_train_batches = 1
keep_recent_epochs = -1

# ------ IO ------
# To train on Borg, consider setting this to always false, so that when the
# job gets stopped and restarted by Borg, it can resume from a checkpoint, in
# which case, you can handle overwriting in your launch script. Set to true
# here for simpler local debugging
overwrite = True
# The following two decide the output directory
outroot = /tmp/tf-training/
xname = lr{lr}_cn{clipnorm}_cv{clipvalue}


# ====== Custom ======
# These parameters are whatever your custom dataset and model require

# ------ Data ------
data_root = /cns/is-d/home/gcam-eng/gcam/interns/xiuming/sim/data/envmaps/outdoor_npz_lh16/

# ------ Model ------
# De facto batch size: number of random rays per gradient step
n_rays_per_step = 1024
light_h = 16
light_model_ckpt = /cns/is-d/home/gcam-eng/gcam/interns/xiuming/sim/output/train/light.ini_envmaps_outdoor_npz_lh16/lr0.001/vis_test/ckpt-50
target_light = /cns/is-d/home/gcam-eng/gcam/interns/xiuming/sim/data/envmaps/outdoor/3023.hdr
loss_transform = log
