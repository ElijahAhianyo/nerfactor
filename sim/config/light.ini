[DEFAULT]

# ====== Must-Have ======
# These parameters are required by the pipeline, regardless of your custom code

# ------ Data ------
dataset = light_latlng
no_batch = True
# bs = 4
cache = True

# ------ Model ------
model = light

# ------ Optimization ------
loss = l2
lr = 5e-4
lr_decay_steps = 500_000
lr_decay_rate = 0.1
clipnorm = -1
clipvalue = -1
epochs = 100

# ------ Logging and Checkpointing ------
ckpt_period = 1
vali_period = 1
vali_batches = 4
vis_train_batches = 4
keep_recent_epochs = -1

# ------ IO ------
# To train on Borg, consider setting this to always false, so that when the
# job gets stopped and restarted by Borg, it can resume from a checkpoint, in
# which case, you can handle overwriting in your launch script. Set to true
# here for simpler local debugging
overwrite = True
# The following two decide the output directory
outroot = /tmp/tf-training/
xname = lr{lr}_cn{clipnorm}_cv{clipvalue}


# ====== Custom ======
# These parameters are whatever your custom dataset and model require

# ------ Data ------
data_root = /cns/is-d/home/gcam-eng/gcam/interns/xiuming/sim/data/envmaps/outdoor_npz_lh16/

# ------ Model ------
pos_enc = True
n_freqs = 8
# De facto batch size: number of random rays per gradient step
n_rays_per_step = 1024
# Latent code
z_dim = 3
z_gauss_mean = 0.
z_gauss_std = 0.01
normalize_z = False
# Loss
loss_transform = log
font_path = /cns/ok-d/home/gcam-eng/gcam/interns/xiuming/relight/data/fonts/open-sans/OpenSans-Regular.ttf

# ------ Network ------
mlp_chunk = 65536
mlp_width = 128
mlp_depth = 8
mlp_skip_at = 4
